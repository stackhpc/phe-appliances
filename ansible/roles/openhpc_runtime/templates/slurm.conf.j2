#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ cluster_name }}
ControlMachine={{ groups['cluster_login'] | first }}
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm
SlurmdSpoolDir=/var/spool/slurm
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
#SlurmctldLogFile=
SlurmdDebug=3
#SlurmdLogFile=
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
# By default, SLURM will log to syslog, which is what we want
#SlurmdLogFile=/var/log/slurm.log
#SlurmctldLogFile=/var/log/slurmctld.log
AccountingStorageType=accounting_storage/filetxt
Epilog=/etc/slurm/slurm.epilog.clean
{% for group in slurm_partitions %}
NodeName={{group.infra}}-{{group.name}}-[0-{{group.num_nodes-1}}] \
    Sockets={{hostvars[groups[group.infra ~ '_' ~ group.name]|first]['ansible_processor_count']}} \
    CoresPerSocket={{hostvars[groups[group.infra ~ '_' ~ group.name]|first]['ansible_processor_cores']}} \
    ThreadsPerCore={{hostvars[groups[group.infra ~ '_' ~ group.name]|first]['ansible_processor_threads_per_core']}} State=UNKNOWN
{% endfor %}
{% for group in slurm_partitions %}
PartitionName={{group.infra}}-{{group.name}} Nodes={{group.infra}}-{{group.name}}-[0-{{group.num_nodes-1}}] Default=YES MaxTime=24:00:00 State=UP
{% endfor %}
# Want nodes that drop out of SLURM's configuration to be automatically 
# returned to service when they come back.
ReturnToService=2
